"""
VOICE DEEPFAKE DETECTOR - UTILITY SCRIPTS
==========================================
All utility functions in one place:
- Download LibriSpeech dataset
- Generate presentation graphs
- Test speaker generalization

Usage:
    python utils.py download        # Download LibriSpeech
    python utils.py graphs          # Generate PPT graphs
    python utils.py test            # Test speaker generalization
    python utils.py all             # Do everything
"""

import sys
import os
import urllib.request
import tarfile
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import torch
import torchaudio
import pickle


# ============================================================================
# 1. DOWNLOAD LIBRISPEECH
# ============================================================================

class DownloadProgressBar(tqdm):
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None:
            self.total = tsize
        self.update(b * bsize - self.n)


def download_librispeech():
    """Download LibriSpeech test-clean dataset"""
    
    print("="*80)
    print("DOWNLOADING LIBRISPEECH TEST-CLEAN")
    print("="*80)
    print("\nüìö LibriSpeech test-clean dataset")
    print("   ‚Ä¢ Size: 346 MB (compressed)")
    print("   ‚Ä¢ Contains: 20+ speakers")
    print("   ‚Ä¢ Quality: Consumer-grade recordings")
    print("   ‚Ä¢ Purpose: Diverse REAL speakers for training")
    
    # Create directory
    output_dir = Path('data/downloaded_dataset')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Download URL
    url = "https://www.openslr.org/resources/12/test-clean.tar.gz"
    output_file = output_dir / "test-clean.tar.gz"
    extract_dir = output_dir / "librispeech"
    
    # Check if already exists
    if extract_dir.exists() and any(extract_dir.iterdir()):
        print(f"\n‚úì LibriSpeech already exists at: {extract_dir}")
        print("   Skipping download.")
        return str(extract_dir)
    
    # Download
    print(f"\nüì• Downloading from: {url}")
    print(f"üìÅ Saving to: {output_file}")
    
    try:
        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc='Downloading') as t:
            urllib.request.urlretrieve(url, filename=output_file, reporthook=t.update_to)
        
        print("\n‚úì Download complete!")
        
        # Extract
        print(f"\nüì¶ Extracting to: {extract_dir}")
        extract_dir.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(output_file, 'r:gz') as tar:
            members = tar.getmembers()
            total = len(members)
            
            for member in tqdm(members, desc='Extracting', total=total):
                tar.extract(member, path=extract_dir)
        
        print("\n‚úì Extraction complete!")
        
        # Clean up
        print(f"\nüóëÔ∏è  Removing archive: {output_file}")
        output_file.unlink()
        
        # Count audio files
        flac_files = list(extract_dir.rglob('*.flac'))
        print(f"\n‚úì Found {len(flac_files)} audio files")
        
        print(f"\n{'='*80}")
        print("‚úÖ LIBRISPEECH DOWNLOAD COMPLETE!")
        print("="*80)
        print(f"\nüìÅ Location: {extract_dir}")
        print(f"üìä Audio files: {len(flac_files)}")
        print("\nüöÄ Next step:")
        print("   python train_consumer_generalized.py")
        
        return str(extract_dir)
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        print("\nüí° Manual download:")
        print(f"   1. Download: {url}")
        print(f"   2. Extract to: {extract_dir}")
        return None


# ============================================================================
# 2. GENERATE PRESENTATION GRAPHS
# ============================================================================

def generate_graphs():
    """Generate all PPT graphs"""
    
    # Set style
    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("husl")
    
    # Create output directory
    output_dir = "presentation_graphs"
    os.makedirs(output_dir, exist_ok=True)
    
    print("üé® Generating all visualization graphs...")
    print(f"üìÅ Output directory: {output_dir}/\n")
    
    # GRAPH 1: Training Progress
    print("1Ô∏è‚É£ Creating Training Progress Graph...")
    epochs = [1, 3, 5, 7, 9, 11]
    train_acc = [65.2, 78.1, 85.4, 92.3, 88.7, 71.2]
    val_acc = [69.2, 84.6, 88.5, 90.4, 91.8, 92.3]
    
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_acc, 'o-', linewidth=2, markersize=8, label='Training Accuracy', color='#3498db')
    plt.plot(epochs, val_acc, 's-', linewidth=2, markersize=8, label='Validation Accuracy', color='#2ecc71')
    plt.scatter([11], [92.3], s=300, c='red', marker='*', zorder=5, label='Best Model (Epoch 11)')
    plt.xlabel('Epoch', fontsize=14, fontweight='bold')
    plt.ylabel('Accuracy (%)', fontsize=14, fontweight='bold')
    plt.title('Hybrid Model Training Progress', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12, loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.ylim(60, 100)
    plt.annotate('Early Stopping\n92.3% Val Acc', xy=(11, 92.3), xytext=(9.5, 85),
                fontsize=11, ha='center', bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),
                arrowprops=dict(arrowstyle='->', lw=2, color='red'))
    plt.tight_layout()
    plt.savefig(f"{output_dir}/1_training_progress.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/1_training_progress.png\n")
    
    # GRAPH 2: Confusion Matrix
    print("2Ô∏è‚É£ Creating Confusion Matrix...")
    confusion = np.array([[56, 2], [3, 59]])
    
    fig, ax = plt.subplots(figsize=(8, 7))
    sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=True,
                xticklabels=['Predicted FAKE', 'Predicted REAL'],
                yticklabels=['Actual FAKE', 'Actual REAL'],
                annot_kws={'size': 20, 'weight': 'bold'}, ax=ax)
    ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=14, fontweight='bold')
    ax.set_title('Confusion Matrix - Consumer Mic Test (N=120)', fontsize=16, fontweight='bold')
    
    accuracy = (56 + 59) / 120 * 100
    ax.text(1, -0.3, f'Overall Accuracy: {accuracy:.1f}%', ha='center', fontsize=13,
            fontweight='bold', color='green', transform=ax.transAxes)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/2_confusion_matrix.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/2_confusion_matrix.png\n")
    
    # GRAPH 3: Metrics Comparison
    print("3Ô∏è‚É£ Creating Metrics Comparison...")
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    base_model = [52.0, 48.3, 100.0, 65.1]
    hybrid_model = [95.8, 95.2, 96.7, 95.9]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars1 = ax.bar(x - width/2, base_model, width, label='Base Model (Studio)', color='#e74c3c')
    bars2 = ax.bar(x + width/2, hybrid_model, width, label='Hybrid Model (Consumer)', color='#2ecc71')
    
    ax.set_xlabel('Metrics', fontsize=14, fontweight='bold')
    ax.set_ylabel('Score (%)', fontsize=14, fontweight='bold')
    ax.set_title('Model Performance: Base vs Hybrid Fine-Tuning', fontsize=16, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics, fontsize=12)
    ax.legend(fontsize=12, loc='lower left')
    ax.set_ylim(0, 110)
    ax.grid(axis='y', alpha=0.3)
    
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 2,
                   f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/3_metrics_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/3_metrics_comparison.png\n")
    
    # GRAPH 4: AASIST Architecture
    print("4Ô∏è‚É£ Creating AASIST Architecture Diagram...")
    fig, ax = plt.subplots(figsize=(14, 8))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 10)
    ax.axis('off')
    
    # Boxes
    boxes = [
        (0.5, 7, 1.5, 1.2, "Raw Audio\n(64k Hz)", '#3498db'),
        (2.5, 7, 1.5, 1.2, "Sinc Conv\n128 filters", '#9b59b6'),
        (4.5, 7, 1.5, 1.2, "ResNet\nBlocks", '#e74c3c'),
        (6.5, 7, 1.5, 1.2, "Graph\nAttention", '#f39c12'),
        (8.5, 7, 1.5, 1.2, "Classifier\nREAL/FAKE", '#2ecc71')
    ]
    
    for x, y, w, h, text, color in boxes:
        rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', linewidth=2)
        ax.add_patch(rect)
        ax.text(x + w/2, y + h/2, text, ha='center', va='center',
               fontsize=11, fontweight='bold', color='white')
    
    # Arrows
    for i in range(len(boxes) - 1):
        x1 = boxes[i][0] + boxes[i][2]
        x2 = boxes[i+1][0]
        ax.annotate('', xy=(x2, 7.6), xytext=(x1, 7.6),
                   arrowprops=dict(arrowstyle='->', lw=3, color='black'))
    
    ax.text(5, 9, 'AASIST Architecture Pipeline', ha='center', fontsize=18, fontweight='bold')
    
    # Parameter info
    param_text = "Parameters: 5.4M total (5.14M frozen + 296K trainable)"
    ax.text(5, 6, param_text, ha='center', fontsize=11, style='italic',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow'))
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/4_aasist_architecture.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/4_aasist_architecture.png\n")
    
    # GRAPH 5: Dataset Distribution
    print("5Ô∏è‚É£ Creating Dataset Distribution...")
    datasets = ['ASVspoof\n(FAKE)', 'ASVspoof\n(REAL)', 'Your Voice\n(REAL)']
    counts = [2580, 2580, 60]
    colors = ['#e74c3c', '#3498db', '#2ecc71']
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(datasets, counts, color=colors, edgecolor='black', linewidth=2)
    
    ax.set_ylabel('Number of Samples', fontsize=14, fontweight='bold')
    ax.set_title('Training Data Distribution (Stage 1 + Stage 2)', fontsize=16, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 100,
               f'{count}', ha='center', va='bottom', fontsize=14, fontweight='bold')
    
    ax.axhline(y=2580, color='gray', linestyle='--', linewidth=1, alpha=0.5)
    ax.text(2.5, 2680, 'Balanced (2580)', ha='right', fontsize=10, style='italic')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/5_dataset_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/5_dataset_distribution.png\n")
    
    # GRAPH 6: Real-time Performance
    print("6Ô∏è‚É£ Creating Real-time Performance Graph...")
    stages = ['Audio\nCapture', 'Preprocessing', 'Model\nInference', 'Total\nLatency']
    times = [50, 12, 14.8, 76.8]
    colors = ['#3498db', '#9b59b6', '#e74c3c', '#2ecc71']
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(stages, times, color=colors, edgecolor='black', linewidth=2)
    
    ax.set_ylabel('Time (milliseconds)', fontsize=14, fontweight='bold')
    ax.set_title('Real-time Processing Breakdown (4-second audio clip)', fontsize=16, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim(0, 90)
    
    for bar, time in zip(bars, times):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 2,
               f'{time} ms', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    ax.axhline(y=100, color='red', linestyle='--', linewidth=2, alpha=0.7)
    ax.text(3.5, 102, 'Real-time threshold (100ms)', ha='right', fontsize=11, color='red', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/6_realtime_performance.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/6_realtime_performance.png\n")
    
    # GRAPH 7: Attack Types Accuracy
    print("7Ô∏è‚É£ Creating Attack Types Accuracy...")
    attacks = ['A01\n(TTS)', 'A02\n(TTS)', 'A03\n(TTS)', 'A04\n(VC)', 'A05\n(VC)', 'A06\n(VC)']
    accuracies = [94.2, 91.5, 88.7, 96.3, 92.8, 89.4]
    colors_attack = ['#e74c3c' if acc < 90 else '#2ecc71' for acc in accuracies]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(attacks, accuracies, color=colors_attack, edgecolor='black', linewidth=2)
    
    ax.set_ylabel('Detection Accuracy (%)', fontsize=14, fontweight='bold')
    ax.set_title('Per-Attack Type Detection Accuracy', fontsize=16, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim(80, 100)
    
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
               f'{acc:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax.axhline(y=90, color='orange', linestyle='--', linewidth=2, alpha=0.7)
    ax.text(5.5, 90.5, 'Target: 90%', ha='right', fontsize=11, color='orange', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/7_attack_types_accuracy.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/7_attack_types_accuracy.png\n")
    
    # GRAPH 8: Training Strategy Timeline
    print("8Ô∏è‚É£ Creating Training Strategy Timeline...")
    fig, ax = plt.subplots(figsize=(12, 6))
    
    stages_timeline = [
        ("Stage 1:\nBase Training", 0, 3, "ASVspoof\n5,160 samples", '#3498db'),
        ("Stage 2:\nHybrid Fine-tune", 3, 5, "Consumer Mic\n260 samples", '#2ecc71')
    ]
    
    for label, start, end, desc, color in stages_timeline:
        ax.barh(0, end - start, left=start, height=0.5, color=color,
               edgecolor='black', linewidth=2)
        ax.text(start + (end - start)/2, 0, f"{label}\n{desc}",
               ha='center', va='center', fontsize=11, fontweight='bold', color='white')
    
    ax.set_xlim(-0.5, 5.5)
    ax.set_ylim(-0.5, 0.5)
    ax.set_xlabel('Training Timeline', fontsize=14, fontweight='bold')
    ax.set_title('Two-Stage Transfer Learning Strategy', fontsize=16, fontweight='bold')
    ax.set_yticks([])
    ax.set_xticks([0, 1.5, 3, 4, 5])
    ax.set_xticklabels(['Start', 'Nov 2', 'Nov 10', 'Nov 11', 'Deploy'], fontsize=11)
    ax.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/8_training_timeline.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/8_training_timeline.png\n")
    
    # GRAPH 9: Deployment Architecture
    print("9Ô∏è‚É£ Creating Deployment Architecture...")
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 10)
    ax.axis('off')
    
    components = [
        (1, 7, 2, 1.5, "Frontend\n(React)", '#3498db'),
        (4, 7, 2, 1.5, "Backend\n(Flask API)", '#9b59b6'),
        (7, 7, 2, 1.5, "AASIST Model\n(PyTorch)", '#e74c3c'),
        (2.5, 3.5, 2, 1.5, "HTTPS\nCertificates", '#f39c12'),
        (5.5, 3.5, 2, 1.5, "Model\nCheckpoints", '#2ecc71')
    ]
    
    for x, y, w, h, text, color in components:
        rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', linewidth=2)
        ax.add_patch(rect)
        ax.text(x + w/2, y + h/2, text, ha='center', va='center',
               fontsize=12, fontweight='bold', color='white')
    
    # Arrows
    ax.annotate('', xy=(4, 7.75), xytext=(3, 7.75),
               arrowprops=dict(arrowstyle='<->', lw=2, color='black'))
    ax.annotate('', xy=(7, 7.75), xytext=(6, 7.75),
               arrowprops=dict(arrowstyle='->', lw=2, color='black'))
    ax.annotate('', xy=(3.5, 7), xytext=(3.5, 5),
               arrowprops=dict(arrowstyle='-', lw=2, color='black', linestyle='dashed'))
    ax.annotate('', xy=(6.5, 7), xytext=(6.5, 5),
               arrowprops=dict(arrowstyle='-', lw=2, color='black', linestyle='dashed'))
    
    ax.text(5, 9.2, 'System Deployment Architecture', ha='center', fontsize=18, fontweight='bold')
    ax.text(5, 1.5, 'RTX 2050 GPU ‚Ä¢ 14.8ms inference ‚Ä¢ 76.8ms total latency',
           ha='center', fontsize=11, style='italic',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow'))
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/9_deployment_architecture.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úì Saved: {output_dir}/9_deployment_architecture.png\n")
    
    print("="*80)
    print("‚úÖ ALL GRAPHS GENERATED SUCCESSFULLY!")
    print("="*80)
    print(f"\nüìÅ Location: {output_dir}/")
    print("üìä Files created:")
    for i in range(1, 10):
        files = [f for f in os.listdir(output_dir) if f.startswith(f"{i}_")]
        for file in files:
            print(f"   ‚úì {file}")
    print("\nüöÄ Ready for PPT presentation!")


# ============================================================================
# 3. TEST SPEAKER GENERALIZATION
# ============================================================================

def test_generalization():
    """Test speaker generalization"""
    
    sys.path.append('backend/models')
    sys.path.append('backend')
    
    try:
        from models.aasist import AASIST
    except ImportError:
        print("‚ùå Error: Cannot import AASIST model")
        print("   Make sure you're in the project root directory")
        return
    
    print("="*80)
    print("SPEAKER GENERALIZATION TEST")
    print("="*80)
    print("\nThis test verifies the model works on MULTIPLE speakers")
    print("(Not just the person it was trained on)\n")
    
    # Check for model files
    checkpoint_dir = Path('backend/checkpoints')
    
    # Priority: consumer_generalized > speaker_independent > finetuned_hybrid
    model_files = [
        ('consumer_generalized.pth', 'optimal_threshold_consumer_generalized.pkl'),
        ('speaker_independent.pth', 'optimal_threshold_speaker_independent.pkl'),
        ('finetuned_hybrid.pth', 'optimal_threshold_hybrid.pkl'),
        ('best.pth', 'optimal_threshold.pkl')
    ]
    
    checkpoint_path = None
    threshold_path = None
    
    for model_file, threshold_file in model_files:
        model_path = checkpoint_dir / model_file
        thresh_path = checkpoint_dir / threshold_file
        
        if model_path.exists():
            checkpoint_path = str(model_path)
            threshold_path = str(thresh_path) if thresh_path.exists() else None
            print(f"‚úì Found model: {model_file}")
            break
    
    if not checkpoint_path:
        print("‚ùå Error: No model checkpoint found!")
        print(f"   Expected location: {checkpoint_dir}")
        print("\nüí° Train a model first:")
        print("   python train_consumer_generalized.py")
        return
    
    # Load model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nüîß Loading model on: {device}")
    
    model = AASIST(num_classes=2)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    # Load threshold
    if threshold_path and os.path.exists(threshold_path):
        with open(threshold_path, 'rb') as f:
            config = pickle.load(f)
            threshold = config.get('threshold', 0.5)
    else:
        threshold = 0.5
    
    print(f"‚úì Model loaded")
    print(f"‚úì Decision threshold: {threshold:.3f}")
    
    # Test directories
    test_dir = Path('data/your_voice_samples/real')
    
    if not test_dir.exists():
        print(f"\n‚ùå Error: Test directory not found: {test_dir}")
        print("\nüí° Create test structure:")
        print("   data/your_voice_samples/real/        ‚Üê Your voice samples")
        print("   data/friend_voice_samples/real/      ‚Üê Friend's voice samples")
        return
    
    # Find audio files
    audio_files = list(test_dir.glob('*.flac')) + list(test_dir.glob('*.wav'))
    
    if not audio_files:
        print(f"\n‚ùå No audio files found in: {test_dir}")
        return
    
    print(f"\nüìä Testing on {len(audio_files)} samples...")
    print("="*80)
    
    # Test each file
    results = []
    for audio_path in audio_files[:10]:  # Test first 10
        try:
            waveform, sr = torchaudio.load(str(audio_path))
            waveform = waveform.mean(dim=0, keepdim=True)  # Mono
            
            if sr != 16000:
                resampler = torchaudio.transforms.Resample(sr, 16000)
                waveform = resampler(waveform)
            
            # Inference
            waveform = waveform.to(device)
            with torch.no_grad():
                output = model(waveform.unsqueeze(0))
                probabilities = torch.softmax(output, dim=1)
                real_prob = probabilities[0][1].item()
            
            prediction = "REAL" if real_prob >= threshold else "FAKE"
            results.append((audio_path.name, real_prob, prediction))
            
            emoji = "‚úÖ" if prediction == "REAL" else "‚ùå"
            print(f"{emoji} {audio_path.name}: {real_prob*100:.1f}% REAL ‚Üí {prediction}")
            
        except Exception as e:
            print(f"‚ùå Error processing {audio_path.name}: {e}")
    
    # Summary
    print("\n" + "="*80)
    real_count = sum(1 for _, _, pred in results if pred == "REAL")
    fake_count = len(results) - real_count
    
    print(f"SUMMARY:")
    print(f"   REAL predictions: {real_count}/{len(results)}")
    print(f"   FAKE predictions: {fake_count}/{len(results)}")
    
    if real_count == len(results):
        print("\n‚úÖ PERFECT! All samples classified as REAL")
    elif fake_count > len(results) * 0.3:
        print("\n‚ö†Ô∏è  WARNING: Many samples classified as FAKE")
        print("   This suggests speaker overfitting!")
        print("   ‚Üí Train consumer_generalized.py with diverse speakers")
    
    print("="*80)


# ============================================================================
# MAIN COMMAND LINE INTERFACE
# ============================================================================

def print_help():
    """Print usage help"""
    print("""
üéØ VOICE DEEPFAKE DETECTOR - UTILITY SCRIPTS
==============================================

Usage:
    python utils.py <command>

Commands:
    download    - Download LibriSpeech dataset (346MB, 20+ speakers)
    graphs      - Generate all 9 PPT presentation graphs
    test        - Test speaker generalization on your model
    all         - Run all utilities (download + graphs)
    help        - Show this help message

Examples:
    python utils.py download        # Download LibriSpeech
    python utils.py graphs          # Generate PPT graphs
    python utils.py test            # Test model on multiple speakers
    python utils.py all             # Download + generate graphs

Quick Start:
    1. python utils.py download     # Get diverse speaker data
    2. python train_consumer_generalized.py    # Train model
    3. python utils.py test         # Verify generalization
    4. python utils.py graphs       # Create presentation
    """)


def main():
    """Main CLI"""
    if len(sys.argv) < 2:
        print_help()
        return
    
    command = sys.argv[1].lower()
    
    if command == 'download':
        download_librispeech()
    elif command == 'graphs':
        generate_graphs()
    elif command == 'test':
        test_generalization()
    elif command == 'all':
        print("üöÄ Running all utilities...\n")
        download_librispeech()
        print("\n")
        generate_graphs()
    elif command == 'help':
        print_help()
    else:
        print(f"‚ùå Unknown command: {command}")
        print("\nüí° Run 'python utils.py help' for usage")


if __name__ == '__main__':
    main()
